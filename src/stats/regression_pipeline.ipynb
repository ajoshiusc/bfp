{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as a template for performing a group difference comparison using BFP and BrainSync. The steps in this pipeline can be easily customized to suite your study. Here, we use data from ADHD200 dataset available through http://fcon_1000.projects.nitrc.org/indi/adhd200/. Specifically, we use the Peking dataset. We will do both univariate and multivariate statistical testing for group differences.\n",
    "\n",
    "The pipeline is written in Python (Jupyter Notebook). We assume thet BrainSuite and BFP are installed on your computer. Install the required python libraries listed below in the script. We recommend using Anaconda python distribution.\n",
    "\n",
    "The steps for running the group comparison are: \n",
    "\n",
    "* Process the fMRI and T1 data of subjects using BFP.\n",
    "* Set the paths in group analysis script.\n",
    "* Run the group analysis script.\n",
    "\n",
    "\n",
    "As an input, we assume that all the subjects data has been preprocessed using BFP. Specifically, we will use the grayordinate data produced by BFP. Also a CSV file containing group labels is assume as an input.\n",
    "\n",
    "First, we use a set of normal control subjects to build an average atlas. Currently, it is done by using BrainSync to synchronize all subject data to an individual and then averaging the synchronized data. In the future, we can use group brainsync included in BFP.\n",
    "\n",
    "Next, we use PCA to reduce the dimensionality in order to keep the data to managable length.\n",
    "\n",
    "Two types of statistical tests are done. \n",
    "1. We compute norm of the difference between synchronized time series of subjects to the atlas. This is the test statistic. A ranksum test is then done on the test statistic to perform group difference comparison. \n",
    "2. The synchronized and PCA reduced time series is used as a test statistic for a multivariate Hotelling test. FDR is used for multiple comparison correction.\n",
    "\n",
    "\n",
    "\n",
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajoshi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ajoshi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as spio\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from surfproc import view_patch_vtk, patch_color_attrib, smooth_surf_function, smooth_patch\n",
    "from dfsio import readdfs\n",
    "import os\n",
    "from brainsync import normalizeData, brainSync\n",
    "from sklearn.decomposition import PCA\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the directories for the data and BFP software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFPPATH = '/home/ajoshi/coding_ground/bfp'\n",
    "BrainSuitePath = '/home/ajoshi/BrainSuite18a/svreg'\n",
    "NDim = 5 # Dimensionality reduction for analysis\n",
    "\n",
    "# study directory where all the grayordinate files lie\n",
    "p_dir = '/deneb_disk/grp_diff/ADHD_Peking_bfp'\n",
    "CSVFILE = '/deneb_disk/ADHD_Peking_bfp/Peking_all_phenotypic.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV file to read the group IDs. This study has three subgroups: \n",
    "1. Normal controls, \n",
    "2. ADHD-hyperactive, and \n",
    "3. ADHD-inattentive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file read\n",
      "There are 234 subjects\n"
     ]
    }
   ],
   "source": [
    "lst = os.listdir(p_dir)\n",
    "\n",
    "# Read CSV File\n",
    "SubIDs = [];reg_var=[];\n",
    "with open(CSVFILE, newline='') as csvfile:    \n",
    "    creader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in creader:\n",
    "        dx = row['DX']\n",
    "        sub = row['ScanDir ID']\n",
    "        qc = row['QC_Rest_1']\n",
    "        rvar = row['Verbal IQ']\n",
    "        fname = os.path.join(p_dir, sub + '_rest_bold.32k.GOrd.mat')\n",
    "\n",
    "        if not os.path.isfile(fname) or int(qc) != 1:\n",
    "            continue\n",
    "\n",
    "        SubIDs.append(sub)\n",
    "        reg_var.append(float(rvar))\n",
    "\n",
    "print('CSV file read\\nThere are %d subjects' % (len(SubIDs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read 100 subjects \n",
    "In this case, we read 100 subjects. Depending on how you have organized the subject directories, you may have to change the path of GOrd.mat files below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,"
     ]
    }
   ],
   "source": [
    "NumSubAtlas = 50\n",
    "LenTime = 235\n",
    "\n",
    "# Read Normal Subjects\n",
    "count1 = 0\n",
    "for sub in SubIDs:\n",
    "    fname = os.path.join(p_dir, sub + '_rest_bold.32k.GOrd.mat')\n",
    "    df = spio.loadmat(fname)\n",
    "    data = df['dtseries'].T\n",
    "    d, _, _ = normalizeData(data)\n",
    "\n",
    "    if count1 == 0:\n",
    "        sub_data = sp.zeros((LenTime, d.shape[1], NumSubArlas))\n",
    "\n",
    "    sub_data[:, :, count1] = d[:LenTime, ]\n",
    "    count1 += 1\n",
    "    print('%d,' %count1, end='')\n",
    "    if count1 == NumSubAtlas:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate average subject \n",
    "An atlas is generated by synchronizing all normal subject's data to one subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Average atlas by synchronizing everyones data to one subject\n",
    "atlas = 0; q=3\n",
    "nSub = NumSubAtlas\n",
    "for ind in range(nSub):\n",
    "    Y2, _ = brainSync(X=sub_data[:, :, q], Y=sub_data[:, :, ind])\n",
    "    atlas += Y2\n",
    "atlas /= (nSub)\n",
    "spio.savemat('avg_atlas.mat', {'atlas':atlas})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn PCA basis\n",
    "Compute PCA basis function from the atlas and use it for dimensionality reduction of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute PCA basis using atlas\n",
    "pca = PCA(n_components=NDim)\n",
    "pca.fit(atlas.T)\n",
    "#print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read normal control subjects for statistical testing\n",
    "Read another set of normal control subjects, separate from the ones that were used for generating the atlas. You may have to adjust the path below of the grayordiate file produced by BFP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,"
     ]
    }
   ],
   "source": [
    "#%% Read 50 Normal Subjects\n",
    "NumSub = 50\n",
    "count1 = 0\n",
    "for sub in SubIDs:\n",
    "    # the line nelow may need editing depending on your dir structure\n",
    "    fname = os.path.join(p_dir, sub + '_rest_bold.32k.GOrd.mat') \n",
    "    df = spio.loadmat(fname)\n",
    "    data = df['dtseries'].T\n",
    "    d, _, _ = normalizeData(data)\n",
    "    if count1 == 0:\n",
    "        sub_data = sp.zeros((LenTime, d.shape[1], NumSub))\n",
    "    sub_data[:, :, count1] = d[:LenTime,]\n",
    "    count1 += 1\n",
    "    print('%d,'%count1, end='')\n",
    "    if count1 == NumSub:\n",
    "        break\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use BrainSync \n",
    "Synchronize the subject data to the atlas and perform PCA of the result. Then compute difference between atlas and the subject. This is the test statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 "
     ]
    }
   ],
   "source": [
    "diff = sp.zeros([sub_data.shape[1],NumSub])\n",
    "fNC = sp.zeros((NDim, sub_data.shape[1], NumSub))\n",
    "for ind in range(NumSub):\n",
    "    Y2, _ = brainSync(X=atlas, Y=sub_data[:, :, ind])\n",
    "    fNC[:, :, ind] = pca.transform(Y2.T).T\n",
    "    diff[:, ind] = sp.sum((Y2 - atlas) ** 2, axis=0)\n",
    "    print('%d '%ind,end='')\n",
    "\n",
    "spio.savemat('diff_avg_atlas.mat', {'diff': diff})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read ADHD Inattentive subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajoshi/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:2400: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/ajoshi/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:2401: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96853\n"
     ]
    }
   ],
   "source": [
    "rcorr = sp.zeros(diff.shape[0])\n",
    "r = sp.array(reg_var[:NumSub])\n",
    "r = sp.absolute(r - sp.mean(r))\n",
    "\n",
    "for nv in range(diff.shape[0]):\n",
    "    a=sp.corrcoef(diff[nv,:],r)\n",
    "    rcorr[nv] = a[0,1]\n",
    "    \n",
    "print(nv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA on the ADHD subjects.\n",
    "Use the same basis that was used for normal controls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Read surfaces for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96854,) (1, 96854)\n"
     ]
    }
   ],
   "source": [
    "lsurf = readdfs(BFPPATH + '/supp_data/bci32kleft.dfs')\n",
    "rsurf = readdfs(BFPPATH + '/supp_data/bci32kright.dfs')\n",
    "a = spio.loadmat(BFPPATH + '/supp_data/USCBrain_grayord_labels.mat')\n",
    "labs = a['labels']\n",
    "\n",
    "lsurf.attributes = np.zeros((lsurf.vertices.shape[0]))\n",
    "rsurf.attributes = np.zeros((rsurf.vertices.shape[0]))\n",
    "lsurf = smooth_patch(lsurf, iterations=1500)\n",
    "rsurf = smooth_patch(rsurf, iterations=1500)\n",
    "labs[sp.isnan(labs)] = 0\n",
    "print(rcorr.shape, labs.shape)\n",
    "rcorr = rcorr*(labs > 0)\n",
    "\n",
    "nVert = lsurf.vertices.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the norm of the difference of Normal Controls from the atlas, at each point on the cortical surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32492,) 32492 (32492, 3)\n"
     ]
    }
   ],
   "source": [
    "lsurf.attributes = rcorr.squeeze()\n",
    "lsurf.attributes = lsurf.attributes[:nVert]\n",
    "rsurf.attributes = rcorr.squeeze()\n",
    "rsurf.attributes = rsurf.attributes[nVert:2*nVert]\n",
    "lsurf = patch_color_attrib(lsurf, clim=[-.5,.5])\n",
    "rsurf = patch_color_attrib(rsurf, clim=[-.5,.5])\n",
    "print(lsurf.attributes.shape, nVert, lsurf.vColor.shape)\n",
    "view_patch_vtk(lsurf, azimuth=100, elevation=180, roll=90, outfile='l1corr.png', show=1)\n",
    "view_patch_vtk(rsurf, azimuth=-100, elevation=180, roll=-90,\n",
    "               outfile='r1corr.png', show=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the norm of the difference of ADHD from the atlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Done!! The outputs are saved as png files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
