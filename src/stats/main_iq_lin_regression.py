# coding: utf-8

# # Regression Pipeline

# This notebook serves as a template for performing a group difference comparison using BFP and BrainSync. The steps in this pipeline can be easily customized to suite your study. Here, we use data from ADHD200 dataset available through http://fcon_1000.projects.nitrc.org/indi/adhd200/. Specifically, we use the Peking dataset. We will correlate cognitive score and fmri signal deviation from normals. This will find the areas that are associated with fMRI signal.
#
# The pipeline is written in Python (Jupyter Notebook). We assume thet BrainSuite and BFP are installed on your computer. Install the required python libraries listed below in the script. We recommend using Anaconda python distribution.
#
# The steps for running the group comparison are:
#
# * Process the fMRI and T1 data of subjects using BFP.
# * Set the paths in group analysis script.
# * Run the regression script.
#
# As an input, we assume that all the subjects data has been preprocessed using BFP. Specifically, we will use the grayordinate data produced by BFP. Also a CSV file containing group labels is assume as an input.
#
# First, we use a set of normal control subjects to build an average atlas. Currently, it is done by using BrainSync to synchronize all subject data to an individual and then averaging the synchronized data. In the future, we can use group brainsync included in BFP.
#
# For a population of subjects, we will compute correlation between norm of synchronized fMRI signal of subjects and atlas, and the cognitive scores of the subjects.
#
# In the future, we will compute multivariate regression.
#
# ### Import the required libraries

# In[1]:

import scipy.io as spio
import scipy as sp
import numpy as np
from surfproc import view_patch_vtk, patch_color_attrib, smooth_surf_function, smooth_patch
from dfsio import readdfs
import os
from brainsync import normalizeData, brainSync
from sklearn.decomposition import PCA
from statsmodels.stats.multitest import fdrcorrection
from stats_utils import read_fcon1000_data, dist2atlas_reg, lin_reg
# ### Set the directories for the data and BFP software
from tqdm import tqdm

# In[2]:

BFPPATH = '/home/ajoshi/coding_ground/bfp'
NDIM = 200  # Dimensionality reduction for analysis

# study directory where all the grayordinate files lie
DATA_DIR = '/deneb_disk/ADHD_Peking_gord'
CSV_FILE = '/deneb_disk/ADHD_Peking_bfp/Peking_all_phenotypic.csv'

# ### Read CSV file to read the group IDs. This study has three subgroups:
# 1. Normal controls,
# 2. ADHD-hyperactive, and
# 3. ADHD-inattentive.

NUM_SUB_ATLAS = 50  # number of subjects for atlas creation
LEN_TIME = 235  # length of the time series
NUM_SUB = 150  # Number of subjects for the study


def main():

    # Read NUM_SUB_ATLAS subjects for creating the atlas
    sub_ids, reg_var, sub_data = read_fcon1000_data(
        csv_fname=CSV_FILE,
        data_dir=DATA_DIR,
        reg_var_name='Verbal IQ',
        num_sub=NUM_SUB_ATLAS,
        len_time=LEN_TIME)

    # ### Generate average subject
    # An atlas is generated by synchronizing all normal subject's data to one subject.

    print('Create Average atlas by synchronizing all subjects to one subject')
    print('Performing Synchronization')
    q = 3
    for ind in tqdm(range(NUM_SUB_ATLAS)):
        Y2, _ = brainSync(X=sub_data[:, :, q], Y=sub_data[:, :, ind])
        if ind == 0:
            avg_atlas = Y2
        else:
            avg_atlas += Y2

    avg_atlas /= (NUM_SUB_ATLAS)
    spio.savemat('avg_atlas.mat', {'avg_atlas': avg_atlas})

    print('Atlas computed and saved')
    # ### Learn PCA basis
    print('PCA done')

    # Read normal control subjects for statistical testing
    print('Reading subjects')

    sub_ids, reg_var, sub_data = read_fcon1000_data(
        csv_fname=CSV_FILE,
        data_dir=DATA_DIR,
        reg_var_name='Verbal IQ',
        num_sub=NUM_SUB,
        len_time=LEN_TIME)

    print('performing stats based on distance to atlas')
    corr_pval, corr_pval_fdr = dist2atlas_reg(
        ref_atlas=avg_atlas, sub_data=sub_data, reg_var=reg_var)

    print('performing stats based on linear regression')
    lin_pval, lin_pval_fdr = lin_reg(
        ref_atlas=avg_atlas, sub_data=sub_data, reg_var=reg_var, ndim=20)

    # In[12]:

    lsurf = readdfs(BFPPATH + '/supp_data/bci32kleft.dfs')
    rsurf = readdfs(BFPPATH + '/supp_data/bci32kright.dfs')
    a = spio.loadmat(BFPPATH + '/supp_data/USCBrain_grayord_labels.mat')
    labs = a['labels']

    lsurf.attributes = np.zeros((lsurf.vertices.shape[0]))
    rsurf.attributes = np.zeros((rsurf.vertices.shape[0]))
    lsurf = smooth_patch(lsurf, iterations=1500)
    rsurf = smooth_patch(rsurf, iterations=1500)
    labs[sp.isnan(labs)] = 0
    print(corr_pval_fdr.shape, labs.shape)
    corr_pval_fdr = corr_pval_fdr * (labs > 0)

    num_vert = lsurf.vertices.shape[0]

    # ### Visualize the norm of the difference of Normal Controls from the atlas, at each point on the cortical surface

    # In[13]:

    lsurf.attributes = 0.05 - corr_pval_fdr.squeeze()
    lsurf.attributes = lsurf.attributes[:num_vert]
    rsurf.attributes = 0.05 - corr_pval_fdr.squeeze()
    rsurf.attributes = rsurf.attributes[num_vert:2 * num_vert]
    lsurf = patch_color_attrib(lsurf, clim=[0, .05])
    rsurf = patch_color_attrib(rsurf, clim=[0, .05])
    print(lsurf.attributes.shape, num_vert, lsurf.vColor.shape)
    view_patch_vtk(
        lsurf,
        azimuth=100,
        elevation=180,
        roll=90,
        outfile='right_corr_pval.png',
        show=1)
    view_patch_vtk(
        rsurf,
        azimuth=-100,
        elevation=180,
        roll=-90,
        outfile='right_corr_pval.png',
        show=1)

    # ### Visualize the norm of the difference of ADHD from the atlas

    # ### All Done!! The outputs are saved as png files.

    lsurf.attributes = 0.05 - lin_pval_fdr.squeeze()
    lsurf.attributes = lsurf.attributes[:num_vert]
    rsurf.attributes = 0.05 - lin_pval_fdr.squeeze()
    rsurf.attributes = rsurf.attributes[num_vert:2 * num_vert]
    lsurf = patch_color_attrib(lsurf, clim=[0, .05])
    rsurf = patch_color_attrib(rsurf, clim=[0, .05])
    print(lsurf.attributes.shape, num_vert, lsurf.vColor.shape)
    view_patch_vtk(
        lsurf,
        azimuth=100,
        elevation=180,
        roll=90,
        outfile='l1_pval.png',
        show=1)
    view_patch_vtk(
        rsurf,
        azimuth=-100,
        elevation=180,
        roll=-90,
        outfile='r1_pavl.png',
        show=1)

    print('Saving results')
    spio.savemat(
        'iq_reg_res.mat', {
            'lsurf': lsurf,
            'rsurf': rsurf,
            'lin_pval': lin_pval,
            'lin_pval_fdr': lin_pval_fdr
        })

    print('Results saved')


if __name__ == "__main__":
    main()