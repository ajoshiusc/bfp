{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Difference Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as a template for performing a group difference comparison using BFP and BrainSync. The steps in this pipeline can be easily customized to suite your study. Here, we use data from ADHD200 dataset available through http://fcon_1000.projects.nitrc.org/indi/adhd200/. Specifically, we use the Peking dataset. We will do both univariate and multivariate statistical testing for group differences.\n",
    "\n",
    "The pipeline is written in Python (Jupyter Notebook). We assume thet BrainSuite and BFP are installed on your computer. Install the required python libraries listed below in the script. We recommend using Anaconda python distribution.\n",
    "\n",
    "The steps for running the group comparison are: \n",
    "\n",
    "* Process the fMRI and T1 data of subjects using BFP.\n",
    "* Set the paths in group analysis script.\n",
    "* Run the group analysis script.\n",
    "\n",
    "\n",
    "As an input, we assume that all the subjects data has been preprocessed using BFP. Specifically, we will use the grayordinate data produced by BFP. Also a CSV file containing group labels is assume as an input.\n",
    "\n",
    "First, we use a set of normal control subjects to build an average atlas. Currently, it is done by using BrainSync to synchronize all subject data to an individual and then averaging the synchronized data. In the future, we can use group brainsync included in BFP.\n",
    "\n",
    "Next, we use PCA to reduce the dimensionality in order to keep the data to managable length.\n",
    "\n",
    "Two types of statistical tests are done. \n",
    "1. We compute norm of the difference between synchronized time series of subjects to the atlas. This is the test statistic. A ranksum test is then done on the test statistic to perform group difference comparison. \n",
    "2. The synchronized and PCA reduced time series is used as a test statistic for a multivariate Hotelling test. FDR is used for multiple comparison correction.\n",
    "\n",
    "\n",
    "\n",
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajoshi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ajoshi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ajoshi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ajoshi/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as spio\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from fmri_methods_sipi import hotelling_t2\n",
    "from surfproc import view_patch_vtk, patch_color_attrib, smooth_surf_function, smooth_patch\n",
    "from dfsio import readdfs\n",
    "import os\n",
    "from brainsync import normalizeData, brainSync\n",
    "from statsmodels.sandbox.stats.multicomp import fdrcorrection0 as FDR\n",
    "from sklearn.decomposition import PCA\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the directories for the data and BFP software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BFPPATH = '/home/ajoshi/coding_ground/bfp'\n",
    "BrainSuitePath = '/home/ajoshi/BrainSuite17a/svreg'\n",
    "NDim = 21 # Dimensionality reduction for analysis\n",
    "\n",
    "# study directory where all the grayordinate files lie\n",
    "p_dir = '/deneb_disk/grp_diff/ADHD_Peking_bfp'\n",
    "CSVFILE = '/deneb_disk/ADHD_Peking_bfp/Peking_all_phenotypic.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV file to read the group IDs. This study has three subgroups: \n",
    "1. Normal controls, \n",
    "2. ADHD-hyperactive, and \n",
    "3. ADHD-inattentive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file read\n",
      "There are 136 Normal, 1 Inattentive ADHD and 61 Hyperactive and 36 Combined subjects\n"
     ]
    }
   ],
   "source": [
    "lst = os.listdir(p_dir)\n",
    "count1 = 0\n",
    "nsub = 0\n",
    "\n",
    "# Read CSV File\n",
    "normSub = [];adhdCombinedSub=[];adhdHyperactiveSub=[];adhdInattentive=[];\n",
    "with open(CSVFILE, newline='') as csvfile:    \n",
    "    creader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in creader:\n",
    "        dx = row['DX']\n",
    "        sub = row['ScanDir ID']\n",
    "        qc = row['QC_Rest_1']\n",
    "        fname = os.path.join(p_dir, sub + '_rest_bold.32k.GOrd.mat')\n",
    "\n",
    "        if not os.path.isfile(fname) or int(qc) != 1:\n",
    "            continue\n",
    "\n",
    "        if int(dx) == 0:\n",
    "            normSub.append(sub)\n",
    "\n",
    "        if int(dx) == 1:\n",
    "            adhdCombinedSub.append(sub)\n",
    "\n",
    "        if int(dx) == 2:\n",
    "            adhdHyperactiveSub.append(sub)\n",
    "\n",
    "        if int(dx) == 3:\n",
    "            adhdInattentive.append(sub)\n",
    "\n",
    "print('CSV file read\\nThere are %d Normal, %d Inattentive ADHD \\\n",
    "and %d Hyperactive and %d Combined subjects' \n",
    "      % (len(normSub), len(adhdInattentive), len(adhdHyperactiveSub), len(adhdCombinedSub)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Normal Controls. \n",
    "In this case, we read 50 normal control subjects. Depending on how you have organized the subject directories, you may have to change the path of GOrd.mat files below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,"
     ]
    }
   ],
   "source": [
    "NumNCforAtlas=50\n",
    "LenTime=235\n",
    "normSubOrig = normSub\n",
    "\n",
    "# Read Normal Subjects\n",
    "normSub = normSub[:NumNCforAtlas]\n",
    "count1 = 0\n",
    "for sub in normSub:\n",
    "    fname = os.path.join(p_dir, sub + '_rest_bold.32k.GOrd.mat')\n",
    "    df = spio.loadmat(fname)\n",
    "    data = df['dtseries'].T\n",
    "    d, _, _ = normalizeData(data)\n",
    "\n",
    "    if count1 == 0:\n",
    "        sub_data = sp.zeros((LenTime, d.shape[1], len(normSub)))\n",
    "\n",
    "    sub_data[:, :, count1] = d[:LenTime, ]\n",
    "    count1 += 1\n",
    "    print('%d,' %count1, end='')\n",
    "    if count1 == NumNCforAtlas:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate average subject \n",
    "An atlas is generated by synchronizing all normal subject's data to one subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Average atlas by synchronizing everyones data to one subject\n",
    "atlas = 0; q=3\n",
    "nSub = len(normSub)\n",
    "for ind in range(nSub):\n",
    "    Y2, _ = brainSync(X=sub_data[:, :, q], Y=sub_data[:, :, ind])\n",
    "    atlas += Y2\n",
    "atlas /= (nSub)\n",
    "spio.savemat('ADHD_avg_atlas.mat', {'atlas':atlas})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn PCA basis\n",
    "Compute PCA basis function from the atlas and use it for dimensionality reduction of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=21, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute PCA basis using atlas\n",
    "pca = PCA(n_components=NDim)\n",
    "pca.fit(atlas.T)\n",
    "#print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read normal control subjects for statistical testing\n",
    "Read another set of normal control subjects, separate from the ones that were used for generating the atlas. You may have to adjust the path below of the grayordiate file produced by BFP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 136 normal controls\n",
      " 50 were used for generating atlas\n",
      " another 50 will be used as controls\n",
      "1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,"
     ]
    }
   ],
   "source": [
    "#%% Read 50 Normal Subjects\n",
    "NumNC = 50\n",
    "print('There are total %d normal controls' % len(normSubOrig))\n",
    "print(' %d were used for generating atlas' % NumNCforAtlas)\n",
    "print(' another %d will be used as controls' % NumNC)\n",
    "normSub = normSubOrig[NumNCforAtlas:]\n",
    "count1 = 0\n",
    "for sub in normSub:\n",
    "    # the line nelow may need editing depending on your dir structure\n",
    "    fname = os.path.join(p_dir, sub + '_rest_bold.32k.GOrd.mat') \n",
    "    df = spio.loadmat(fname)\n",
    "    data = df['dtseries'].T\n",
    "    d, _, _ = normalizeData(data)\n",
    "    if count1 == 0:\n",
    "        sub_data = sp.zeros((LenTime, d.shape[1], NumNC))\n",
    "    sub_data[:, :, count1] = d[:LenTime,]\n",
    "    count1 += 1\n",
    "    print('%d,'%count1, end='')\n",
    "    if count1 == NumNC:\n",
    "        break\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use BrainSync \n",
    "Synchronize the subject data to the atlas and perform PCA of the result. Then compute difference between atlas and the subject. This is the test statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 "
     ]
    }
   ],
   "source": [
    "diff = sp.zeros([sub_data.shape[1],50])\n",
    "fNC = sp.zeros((NDim, sub_data.shape[1], 50))\n",
    "for ind in range(50):\n",
    "    Y2, _ = brainSync(X=atlas, Y=sub_data[:, :, ind])\n",
    "    fNC[:, :, ind] = pca.transform(Y2.T).T\n",
    "    diff[:, ind] = sp.sum((Y2 - atlas) ** 2, axis=0)\n",
    "    print('%d '%ind,end='')\n",
    "\n",
    "spio.savemat('ADHD_diff_avg_atlas.mat', {'diff': diff})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read ADHD Inattentive subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 "
     ]
    }
   ],
   "source": [
    "# Read ADHD inattentive\n",
    "count1 = 0\n",
    "for sub in adhdInattentive:\n",
    "    fname = os.path.join(p_dir, sub + '_rest_bold.32k.GOrd.mat')\n",
    "    df = spio.loadmat(fname)\n",
    "    data = df['dtseries'].T\n",
    "    d, _, _ = normalizeData(data)\n",
    "    if count1 == 0:\n",
    "        sub_data = sp.zeros((235, d.shape[1], 50))\n",
    "    sub_data[:, :, count1] = d[:LenTime,]\n",
    "    count1 += 1\n",
    "    print('%d '%count1, end='')\n",
    "    if count1 == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA on the ADHD subjects.\n",
    "Use the same basis that was used for normal controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 "
     ]
    }
   ],
   "source": [
    "#%% Atlas to normal subjects diff & Do PCA of ADHD\n",
    "diffAdhdInatt = sp.zeros([sub_data.shape[1],50])\n",
    "fADHD = sp.zeros((NDim, sub_data.shape[1], 50))\n",
    "\n",
    "for ind in range(50):\n",
    "    Y2, _ = brainSync(X=atlas, Y=sub_data[:, :, ind])\n",
    "    fADHD[:, :, ind] = pca.transform(Y2.T).T\n",
    "    diffAdhdInatt[:, ind] = sp.sum((Y2 - atlas) ** 2, axis=0)\n",
    "    print('%d '%ind, end='')\n",
    "\n",
    "spio.savemat('ADHD_diff_adhd_inattentive.mat', {'diffAdhdInatt': diffAdhdInatt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Read surfaces for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsurf = readdfs(BFPPATH+'/supp_data/bci32kleft.dfs')\n",
    "rsurf = readdfs(BFPPATH+'/supp_data/bci32kright.dfs')\n",
    "a=spio.loadmat(BFPPATH+'/supp_data/USCBrain_grayord_labels.mat')\n",
    "labs=a['labels']\n",
    "\n",
    "lsurf.attributes = np.zeros((lsurf.vertices.shape[0]))\n",
    "rsurf.attributes = np.zeros((rsurf.vertices.shape[0]))\n",
    "lsurf=smooth_patch(lsurf,iterations=1500)\n",
    "rsurf=smooth_patch(rsurf,iterations=1500)\n",
    "labs[sp.isnan(labs)]=0\n",
    "diff=diff*(labs.T>0)\n",
    "diffAdhdInatt=diffAdhdInatt*(labs.T>0)\n",
    "\n",
    "nVert = lsurf.vertices.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the norm of the difference of Normal Controls from the atlas, at each point on the cortical surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsurf.attributes = np.sqrt(np.sum((diff), axis=1))\n",
    "lsurf.attributes = lsurf.attributes[:nVert]/50\n",
    "rsurf.attributes = np.sqrt(np.sum((diff), axis=1))\n",
    "rsurf.attributes = rsurf.attributes[nVert:2*nVert]/50\n",
    "lsurf = patch_color_attrib(lsurf, clim=[0,.15])\n",
    "rsurf = patch_color_attrib(rsurf, clim=[0,.15])\n",
    "\n",
    "view_patch_vtk(lsurf, azimuth=100, elevation=180, roll=90,\n",
    "               outfile='l1normal.png', show=1)\n",
    "view_patch_vtk(rsurf, azimuth=-100, elevation=180, roll=-90,\n",
    "               outfile='r1normal.png', show=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the norm of the difference of ADHD from the atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsurf.attributes = np.sqrt(np.sum((diffAdhdInatt), axis=1))\n",
    "lsurf.attributes = lsurf.attributes[:nVert]/50\n",
    "rsurf.attributes = np.sqrt(np.sum((diffAdhdInatt), axis=1))\n",
    "rsurf.attributes = rsurf.attributes[nVert:2*nVert]/50\n",
    "lsurf = patch_color_attrib(lsurf, clim=[0, .15])\n",
    "rsurf = patch_color_attrib(rsurf, clim=[0, .15])\n",
    "\n",
    "view_patch_vtk(lsurf, azimuth=100, elevation=180, roll=90,\n",
    "               outfile='l1adhd.png', show=1)\n",
    "view_patch_vtk(rsurf, azimuth=-100, elevation=180, roll=-90,\n",
    "               outfile='r1adhd.png', show=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between ADHD to atlas and Normal controls to atlas differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsurf.attributes = np.sqrt(np.sum((diffAdhdInatt), axis=1))-np.sqrt(np.sum((diff), axis=1))\n",
    "rsurf.attributes = np.sqrt(np.sum((diffAdhdInatt), axis=1))-np.sqrt(np.sum((diff), axis=1))\n",
    "lsurf.attributes = lsurf.attributes[:nVert]/50\n",
    "rsurf.attributes = rsurf.attributes[nVert:2*nVert]/50\n",
    "\n",
    "#lsurf.attributes = smooth_surf_function(lsurf,lsurf.attributes,1,1)\n",
    "#rsurf.attributes = smooth_surf_function(rsurf,rsurf.attributes,1,1)\n",
    "lsurf = patch_color_attrib(lsurf, clim=[-0.01, 0.01])\n",
    "rsurf = patch_color_attrib(rsurf, clim=[-0.01, 0.01])\n",
    "\n",
    "view_patch_vtk(lsurf, azimuth=100, elevation=180, roll=90,\n",
    "               outfile='l1adhd_normal_diff.png', show=1)\n",
    "view_patch_vtk(rsurf, azimuth=-100, elevation=180, roll=-90,\n",
    "               outfile='r1adhd_normal_diff.png', show=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranksum test on these differences. \n",
    "FDR is used for multiple comparison correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = sp.zeros(diff.shape[0])\n",
    "for vind in range(diff.shape[0]):\n",
    "    _, pv[vind] = sp.stats.ranksums(diff[vind,:], diffAdhdInatt[vind,:])\n",
    "\n",
    "\n",
    "t, pvfdr = FDR(pv[labs[0, :] > 0])\n",
    "\n",
    "lsurf.attributes = 1-pv\n",
    "rsurf.attributes = 1-pv\n",
    "lsurf.attributes = lsurf.attributes[:nVert]\n",
    "rsurf.attributes = rsurf.attributes[nVert:2*nVert]\n",
    "#lsurf.attributes = smooth_surf_function(lsurf, lsurf.attributes, .3, .3)\n",
    "#rsurf.attributes = smooth_surf_function(rsurf, rsurf.attributes, .3, .3)\n",
    "\n",
    "lsurf = patch_color_attrib(lsurf, clim=[0.95, 1.0])\n",
    "rsurf = patch_color_attrib(rsurf, clim=[0.95, 1.0])\n",
    "\n",
    "view_patch_vtk(lsurf, azimuth=-90, elevation=180, roll=-90,\n",
    "               outfile='l1adhd_normal_pval.png', show=1)\n",
    "view_patch_vtk(lsurf, azimuth=100, elevation=180, roll=90,\n",
    "               outfile='l2adhd_normal_pval.png', show=1)\n",
    "\n",
    "view_patch_vtk(rsurf, azimuth=90, elevation=180, roll=90,\n",
    "               outfile='r1adhd_normal_pval.png', show=1)\n",
    "view_patch_vtk(rsurf, azimuth=-100, elevation=180, roll=-90,\n",
    "               outfile='r2adhd_normal_pval.png', show=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Hotelling test on the synchronized data for ADHD and normal controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = sp.transpose(fADHD, axes=[0, 2, 1])\n",
    "fc = sp.transpose(fNC, axes=[0, 2, 1])\n",
    "\n",
    "labs=sp.squeeze(labs)\n",
    "pv, t2 = hotelling_t2(fa[:, :, (labs > 0)], fc[:, :, (labs > 0)])\n",
    "lsurf.attributes=sp.zeros((labs.shape[0]))\n",
    "lsurf.attributes[labs>0] = 1.0 - pv\n",
    "lsurf.attributes = smooth_surf_function(lsurf, lsurf.attributes[:nVert], 1, 1)\n",
    "lsurf = patch_color_attrib(lsurf, clim=[0.95, 1.0])\n",
    "view_patch_vtk(lsurf, azimuth=90, elevation=180, roll=90,\n",
    "               outfile='l1multiadhd_normal_pval.png', show=1)\n",
    "view_patch_vtk(lsurf, azimuth=-100, elevation=180, roll=-90,\n",
    "               outfile='l2multiadhd_normal_pval.png', show=1)\n",
    "\n",
    "rsurf.attributes = sp.zeros((labs.shape[0]))\n",
    "rsurf.attributes[labs > 0] = 1.0 - pv\n",
    "rsurf.attributes = smooth_surf_function(rsurf,\n",
    "                                        rsurf.attributes[nVert:2*nVert], 1, 1)\n",
    "rsurf = patch_color_attrib(rsurf, clim=[0.95, 1.0])\n",
    "view_patch_vtk(rsurf, azimuth=90, elevation=180, roll=90,\n",
    "               outfile='r1multiadhd_normal_pval.png', show=1)\n",
    "view_patch_vtk(rsurf, azimuth=-100, elevation=180, roll=-90,\n",
    "               outfile='r2multiadhd_normal_pval.png', show=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
