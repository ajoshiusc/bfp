# coding: utf-8

# # Regression Pipeline

# This notebook serves as a template for performing a group difference comparison using BFP and BrainSync. The steps in this pipeline can be easily customized to suite your study. Here, we use data from ADHD200 dataset available through http://fcon_1000.projects.nitrc.org/indi/adhd200/. Specifically, we use the Peking dataset. We will correlate cognitive score and fmri signal deviation from normals. This will find the areas that are associated with fMRI signal.
#
# The pipeline is written in Python (Jupyter Notebook). We assume thet BrainSuite and BFP are installed on your computer. Install the required python libraries listed below in the script. We recommend using Anaconda python distribution.
#
# The steps for running the group comparison are:
#
# * Process the fMRI and T1 data of subjects using BFP.
# * Set the paths in group analysis script.
# * Run the regression script.
#
# As an input, we assume that all the subjects data has been preprocessed using BFP. Specifically, we will use the grayordinate data produced by BFP. Also a CSV file containing group labels is assume as an input.
#
# First, we use a set of normal control subjects to build an average atlas. Currently, it is done by using BrainSync to synchronize all subject data to an individual and then averaging the synchronized data. In the future, we can use group brainsync included in BFP.
#
# For a population of subjects, we will compute correlation between norm of synchronized fMRI signal of subjects and atlas, and the cognitive scores of the subjects.
#
# In the future, we will compute multivariate regression.
#
# ### Import the required libraries

# In[1]:

import scipy.io as spio
import scipy as sp
import numpy as np
from surfproc import view_patch_vtk, patch_color_attrib, smooth_surf_function, smooth_patch
from dfsio import readdfs
import os
from brainsync import normalizeData, brainSync
from sklearn.decomposition import PCA
import csv
import statsmodels.api as sm
from statsmodels.stats.multitest import fdrcorrection
# ### Set the directories for the data and BFP software

# In[2]:

BFPPATH = '/home/ajoshi/coding_ground/bfp'
BrainSuitePath = '/home/ajoshi/BrainSuite18a/svreg'
NDim = 200  # Dimensionality reduction for analysis

# study directory where all the grayordinate files lie
p_dir = '/deneb_disk/ADHD_Peking_gord'
CSVFILE = '/deneb_disk/ADHD_Peking_bfp/Peking_all_phenotypic.csv'

# ### Read CSV file to read the group IDs. This study has three subgroups:
# 1. Normal controls,
# 2. ADHD-hyperactive, and
# 3. ADHD-inattentive.

# In[3]:

lst = os.listdir(p_dir)

# Read CSV File
SubIDs = []
reg_var = []
with open(CSVFILE, newline='') as csvfile:
    creader = csv.DictReader(csvfile, delimiter=',', quotechar='"')
    for row in creader:
        dx = row['DX']
        sub = row['ScanDir ID']
        qc = row['QC_Rest_1']
        rvar = row['Verbal IQ']
        fname = os.path.join(p_dir, sub + '_rest_bold.32k.GOrd.filt.mat')

        if not os.path.isfile(fname) or int(qc) != 1:
            continue

        SubIDs.append(sub)
        reg_var.append(float(rvar))

print('CSV file read\nThere are %d subjects' % (len(SubIDs)))

# ### Read 100 subjects
# In this case, we read 100 subjects. Depending on how you have organized the subject directories, you may have to change the path of GOrd.mat files below.

# In[4]:

NumSubAtlas = 50
LenTime = 235

# Read Normal Subjects
count1 = 0
for sub in SubIDs:
    fname = os.path.join(p_dir, sub + '_rest_bold.32k.GOrd.filt.mat')
    df = spio.loadmat(fname)
    data = df['dtseries'].T
    d, _, _ = normalizeData(data)

    if count1 == 0:
        sub_data = sp.zeros((LenTime, d.shape[1], NumSubAtlas))

    sub_data[:, :, count1] = d[:LenTime, ]
    count1 += 1
    print('%d,' % count1, end='')
    if count1 == NumSubAtlas:
        break

# ### Generate average subject
# An atlas is generated by synchronizing all normal subject's data to one subject.

# In[5]:

# Create Average atlas by synchronizing everyones data to one subject
atlas = 0
q = 3
nSub = NumSubAtlas
for ind in range(nSub):
    Y2, _ = brainSync(X=sub_data[:, :, q], Y=sub_data[:, :, ind])
    atlas += Y2
atlas /= (nSub)
spio.savemat('avg_atlas.mat', {'atlas': atlas})

print('Atlas computed and saved')
# ### Learn PCA basis
# Compute PCA basis function from the atlas and use it for dimensionality reduction of the data.

# In[6]:

# Compute PCA basis using atlas
pca = PCA(n_components=NDim)
pca.fit(atlas.T)
#print(pca.explained_variance_ratio_)

# ### Read normal control subjects for statistical testing
# Read another set of normal control subjects, separate from the ones that were used for generating the atlas. You may have to adjust the path below of the grayordiate file produced by BFP.

# In[7]:

#%% Read 50 Normal Subjects
print('Reading subjects')

NumSub = 150
count1 = 0
for sub in SubIDs:
    # the line nelow may need editing depending on your dir structure
    fname = os.path.join(p_dir, sub + '_rest_bold.32k.GOrd.filt.mat')
    df = spio.loadmat(fname)
    data = df['dtseries'].T
    d, _, _ = normalizeData(data)
    if count1 == 0:
        sub_data = sp.zeros((LenTime, d.shape[1], NumSub))
    sub_data[:, :, count1] = d[:LenTime, ]
    count1 += 1
    print('%d,' % count1, end='')
    if count1 == NumSub:
        break

# ### Use BrainSync
# Synchronize the subject data to the atlas and perform PCA of the result. Then compute difference between atlas and the subject. This is the test statistic.

# In[8]:

diff = sp.zeros([sub_data.shape[1], NumSub])
rData = sp.zeros((NDim, sub_data.shape[1], NumSub))
for ind in range(NumSub):
    Y2, _ = brainSync(X=atlas, Y=sub_data[:, :, ind])
    rData[:, :, ind] = pca.transform(Y2.T).T
    diff[:, ind] = sp.sum((Y2 - atlas)**2, axis=0)
    print('%d ' % ind, end='')

spio.savemat('diff_avg_atlas.mat', {'diff': diff})

# In[9]:

rcorr = sp.zeros(diff.shape[0])
r = sp.array(reg_var[:NumSub])
r = sp.absolute(r - sp.mean(r))
pval_surf = sp.zeros(diff.shape[0])

for nv in range(diff.shape[0]):
    a = sp.corrcoef(diff[nv, :], r)
    rcorr[nv] = a[0, 1]

    X = rData[:, nv, :]
    X = sm.add_constant(X.T)
    est = sm.OLS(r, X)
    pval_surf[nv] = est.fit().f_pvalue

print('Regression is done')

m = np.isnan(pval_surf)
pval_surf[m] = .5
_, pval_surf_corr = fdrcorrection(pval_surf)
#  ### Read surfaces for visualization

# In[12]:

lsurf = readdfs(BFPPATH + '/supp_data/bci32kleft.dfs')
rsurf = readdfs(BFPPATH + '/supp_data/bci32kright.dfs')
a = spio.loadmat(BFPPATH + '/supp_data/USCBrain_grayord_labels.mat')
labs = a['labels']

lsurf.attributes = np.zeros((lsurf.vertices.shape[0]))
rsurf.attributes = np.zeros((rsurf.vertices.shape[0]))
lsurf = smooth_patch(lsurf, iterations=1500)
rsurf = smooth_patch(rsurf, iterations=1500)
labs[sp.isnan(labs)] = 0
print(rcorr.shape, labs.shape)
rcorr = rcorr * (labs > 0)

nVert = lsurf.vertices.shape[0]

# ### Visualize the norm of the difference of Normal Controls from the atlas, at each point on the cortical surface

# In[13]:

lsurf.attributes = rcorr.squeeze()
lsurf.attributes = lsurf.attributes[:nVert]
rsurf.attributes = rcorr.squeeze()
rsurf.attributes = rsurf.attributes[nVert:2 * nVert]
lsurf = patch_color_attrib(lsurf, clim=[-.5, .5])
rsurf = patch_color_attrib(rsurf, clim=[-.5, .5])
print(lsurf.attributes.shape, nVert, lsurf.vColor.shape)
view_patch_vtk(
    lsurf, azimuth=100, elevation=180, roll=90, outfile='l1corr.png', show=1)
view_patch_vtk(
    rsurf, azimuth=-100, elevation=180, roll=-90, outfile='r1corr.png', show=1)

# ### Visualize the norm of the difference of ADHD from the atlas

# ### All Done!! The outputs are saved as png files.

lsurf.attributes = 0.05 - pval_surf_corr.squeeze()
lsurf.attributes = lsurf.attributes[:nVert]
rsurf.attributes = 0.05 - pval_surf_corr.squeeze()
rsurf.attributes = rsurf.attributes[nVert:2 * nVert]
lsurf = patch_color_attrib(lsurf, clim=[0, .05])
rsurf = patch_color_attrib(rsurf, clim=[0, .05])
print(lsurf.attributes.shape, nVert, lsurf.vColor.shape)
view_patch_vtk(
    lsurf, azimuth=100, elevation=180, roll=90, outfile='l1_pval.png', show=1)
view_patch_vtk(
    rsurf,
    azimuth=-100,
    elevation=180,
    roll=-90,
    outfile='r1_pavl.png',
    show=1)

# ### Visualize the norm of the difference of ADHD from the atlas
lsurf = patch_color_attrib(lsurf, clim=[0, .5])
rsurf = patch_color_attrib(rsurf, clim=[0, .5])
print(lsurf.attributes.shape, nVert, lsurf.vColor.shape)
view_patch_vtk(
    lsurf,
    azimuth=100,
    elevation=180,
    roll=90,
    outfile='l1_pval_p5.png',
    show=1)
view_patch_vtk(
    rsurf,
    azimuth=-100,
    elevation=180,
    roll=-90,
    outfile='r1_pavl_p5.png',
    show=1)

print('Saving results')
spio.savemat(
    'iq_reg_res.mat', {
        'lsurf': lsurf,
        'rsurf': rsurf,
        'pval_surf': pval_surf,
        'pval_surf_corr': pval_surf_corr
    })

print('Results saved')
